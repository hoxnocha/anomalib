{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/scratch/tyang/miniconda3/env/anomalib_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np  \n",
    "from matplotlib import pyplot as plt \n",
    "from PIL import Image    \n",
    "import torch\n",
    "\n",
    "from anomalib.config import get_configurable_parameters\n",
    "from anomalib.data import get_datamodule\n",
    "from anomalib.models import get_model\n",
    "from anomalib.models.components import feature_extractors\n",
    "import torchvision\n",
    "from anomalib.models.components.feature_extractors import TorchFXFeatureExtractor\n",
    "from torchvision.models.densenet import DenseNet201_Weights\n",
    "import torch.nn.functional as F\n",
    "from anomalib.models.components.cluster.kmeans import KMeans\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset:\n",
      "  name: airogs\n",
      "  format: airogs\n",
      "  path: /home/students/tyang/yolov5results\n",
      "  task: classification # options: [classification, segmentation]\n",
      "  category: cat0/crops/od\n",
      "  pre_selection: False\n",
      "  number_of_samples: 17999\n",
      "  train_batch_size: 32\n",
      "  eval_batch_size: 32\n",
      "  num_workers: 8\n",
      "  image_size: 256 # dimensions to which images are resized (mandatory)\n",
      "  center_crop:  # dimensions to which images are center-cropped after resizing (optional)\n",
      "  normalization: imagenet # data distribution to which the images will be normalized: [none, imagenet]\n",
      "  transform_config:\n",
      "    train: null\n",
      "    eval: null\n",
      "  test_split_mode: from_dir # options: [from_dir, synthetic]\n",
      "  test_split_ratio: 0.1 # fraction of train images held out testing (usage depends on test_split_mode)\n",
      "  val_split_mode: same_as_test # options: [same_as_test, from_test, synthetic]\n",
      "  val_split_ratio: 0.1 # fraction of train/test images held out for validation (usage depends on val_split_mode)\n",
      "\n",
      "  tiling:\n",
      "    apply: false\n",
      "    tile_size: null\n",
      "    stride: null\n",
      "    remove_border_count: 0\n",
      "    use_random_tiling: False\n",
      "    random_tile_count: 16\n",
      "\n",
      "model:\n",
      "  name: patchcore\n",
      "  backbone: wide_resnet50_2\n",
      "  pre_trained: true\n",
      "  layers:\n",
      "    - layer2\n",
      "    - layer3\n",
      "    \n",
      "  coreset_sampling_ratio: 0.2\n",
      "\n",
      "  num_neighbors: 5\n",
      "  normalization_method: min_max # options: [null, min_max, cdf]\n",
      "\n",
      "metrics:\n",
      "  image:\n",
      "    - F1Score\n",
      "    - AUROC\n",
      "  pixel:\n",
      "    - F1Score\n",
      "    - AUROC\n",
      "  threshold:\n",
      "    method: adaptive #options: [adaptive, manual]\n",
      "    manual_image: null\n",
      "    manual_pixel: null\n",
      "\n",
      "visualization:\n",
      "  show_images: False # show images on the screen\n",
      "  save_images: True # save images to the file system\n",
      "  log_images: True # log images to the available loggers (if any)\n",
      "  image_save_path: null # path to which images will be saved\n",
      "  mode: full # options: [\"full\", \"simple\"]\n",
      "\n",
      "project:\n",
      "  seed: 0\n",
      "  path: /work/scratch/tyang/anomalib/\n",
      "\n",
      "logging:\n",
      "  logger: [] # options: [comet, tensorboard, wandb, csv] or combinations.\n",
      "  log_graph: false # Logs the model graph to respective logger.\n",
      "\n",
      "optimization:\n",
      "  export_mode: null # options: onnx, openvino\n",
      "\n",
      "# PL Trainer Args. Don't add extra parameter here.\n",
      "trainer:\n",
      "  enable_checkpointing: true\n",
      "  default_root_dir: null\n",
      "  gradient_clip_val: 0\n",
      "  gradient_clip_algorithm: norm\n",
      "  num_nodes: 1\n",
      "  devices: 1\n",
      "  enable_progress_bar: true\n",
      "  overfit_batches: 0.0\n",
      "  track_grad_norm: -1\n",
      "  check_val_every_n_epoch: 1 # Don't validate before extracting features.\n",
      "  fast_dev_run: false\n",
      "  accumulate_grad_batches: 1\n",
      "  max_epochs: 1\n",
      "  min_epochs: null\n",
      "  max_steps: -1\n",
      "  min_steps: null\n",
      "  max_time: null\n",
      "  limit_train_batches: 1.0\n",
      "  limit_val_batches: 1.0\n",
      "  limit_test_batches: 1.0\n",
      "  limit_predict_batches: 1.0\n",
      "  val_check_interval: 1.0 # Don't validate before extracting features.\n",
      "  log_every_n_steps: 50\n",
      "  accelerator: auto # <\"cpu\", \"gpu\", \"tpu\", \"ipu\", \"hpu\", \"auto\">\n",
      "  strategy: null\n",
      "  sync_batchnorm: false\n",
      "  precision: 32\n",
      "  enable_model_summary: true\n",
      "  num_sanity_val_steps: 0\n",
      "  profiler: null\n",
      "  benchmark: false\n",
      "  deterministic: false\n",
      "  reload_dataloaders_every_n_epochs: 0\n",
      "  auto_lr_find: false\n",
      "  replace_sampler_ddp: true\n",
      "  detect_anomaly: false\n",
      "  auto_scale_batch_size: false\n",
      "  plugins: null\n",
      "  move_metrics_to_cpu: false\n",
      "  multiple_trainloader_mode: max_size_cycle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/students/tyang/anomalib/src/anomalib/config/config.py:243: UserWarning: The seed value is now fixed to 0. Up to v0.3.7, the seed was not fixed when the seed value was set to 0. If you want to use the random seed, please select `None` for the seed value (`null` in the YAML file) or remove the `seed` key from the YAML file.\n",
      "  warn(\n",
      "/home/students/tyang/anomalib/src/anomalib/config/config.py:280: UserWarning: config.project.unique_dir is set to False. This does not ensure that your results will be written in an empty directory and you may overwrite files.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "MODEL = \"patchcore\"\n",
    "CONFIG_similiar_PATH = f\"/home/students/tyang/anomalib/src/anomalib/models/{MODEL}/config_similiar.yaml\"\n",
    "with open(file=CONFIG_similiar_PATH, mode=\"r\",encoding=\"utf-8\") as f:\n",
    "    print(f.read())\n",
    "    \n",
    "config_similiar = get_configurable_parameters(config_path=CONFIG_similiar_PATH)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset:\n",
      "  name: airogs\n",
      "  format: airogs\n",
      "  path: /home/students/tyang/yolov5results\n",
      "  task: classification # options: [classification, segmentation]\n",
      "  category: cat0/crops/od\n",
      "  pre_selection: True\n",
      "  number_of_samples: 9\n",
      "  train_batch_size: 1\n",
      "  eval_batch_size: 1\n",
      "  num_workers: 8\n",
      "  image_size: 256 # dimensions to which images are resized (mandatory)\n",
      "  center_crop:  # dimensions to which images are center-cropped after resizing (optional)\n",
      "  normalization: imagenet # data distribution to which the images will be normalized: [none, imagenet]\n",
      "  transform_config:\n",
      "    train: null\n",
      "    eval: null\n",
      "  test_split_mode: from_dir # options: [from_dir, synthetic]\n",
      "  test_split_ratio: 0.001 # fraction of train images held out testing (usage depends on test_split_mode)\n",
      "  val_split_mode: same_as_test # options: [same_as_test, from_test, synthetic]\n",
      "  val_split_ratio: 0.001 # fraction of train/test images held out for validation (usage depends on val_split_mode)\n",
      "\n",
      "  tiling:\n",
      "    apply: false\n",
      "    tile_size: null\n",
      "    stride: null\n",
      "    remove_border_count: 0\n",
      "    use_random_tiling: False\n",
      "    random_tile_count: 16\n",
      "\n",
      "model:\n",
      "  name: patchcore\n",
      "  backbone: wide_resnet50_2\n",
      "  pre_trained: true\n",
      "  layers:\n",
      "    - layer2\n",
      "    - layer3\n",
      "    \n",
      "  coreset_sampling_ratio: 0.2\n",
      "\n",
      "  num_neighbors: 5\n",
      "  normalization_method: min_max # options: [null, min_max, cdf]\n",
      "\n",
      "metrics:\n",
      "  image:\n",
      "    - F1Score\n",
      "    - AUROC\n",
      "  pixel:\n",
      "    - F1Score\n",
      "    - AUROC\n",
      "  threshold:\n",
      "    method: adaptive #options: [adaptive, manual]\n",
      "    manual_image: null\n",
      "    manual_pixel: null\n",
      "\n",
      "visualization:\n",
      "  show_images: False # show images on the screen\n",
      "  save_images: True # save images to the file system\n",
      "  log_images: True # log images to the available loggers (if any)\n",
      "  image_save_path: null # path to which images will be saved\n",
      "  mode: full # options: [\"full\", \"simple\"]\n",
      "\n",
      "project:\n",
      "  seed: 0\n",
      "  path: /work/scratch/tyang/anomalib/\n",
      "\n",
      "logging:\n",
      "  logger: [] # options: [comet, tensorboard, wandb, csv] or combinations.\n",
      "  log_graph: false # Logs the model graph to respective logger.\n",
      "\n",
      "optimization:\n",
      "  export_mode: null # options: onnx, openvino\n",
      "\n",
      "# PL Trainer Args. Don't add extra parameter here.\n",
      "trainer:\n",
      "  enable_checkpointing: true\n",
      "  default_root_dir: null\n",
      "  gradient_clip_val: 0\n",
      "  gradient_clip_algorithm: norm\n",
      "  num_nodes: 1\n",
      "  devices: 1\n",
      "  enable_progress_bar: true\n",
      "  overfit_batches: 0.0\n",
      "  track_grad_norm: -1\n",
      "  check_val_every_n_epoch: 1 # Don't validate before extracting features.\n",
      "  fast_dev_run: false\n",
      "  accumulate_grad_batches: 1\n",
      "  max_epochs: 1\n",
      "  min_epochs: null\n",
      "  max_steps: -1\n",
      "  min_steps: null\n",
      "  max_time: null\n",
      "  limit_train_batches: 1.0\n",
      "  limit_val_batches: 1.0\n",
      "  limit_test_batches: 1.0\n",
      "  limit_predict_batches: 1.0\n",
      "  val_check_interval: 1.0 # Don't validate before extracting features.\n",
      "  log_every_n_steps: 50\n",
      "  accelerator: auto # <\"cpu\", \"gpu\", \"tpu\", \"ipu\", \"hpu\", \"auto\">\n",
      "  strategy: null\n",
      "  sync_batchnorm: false\n",
      "  precision: 32\n",
      "  enable_model_summary: true\n",
      "  num_sanity_val_steps: 0\n",
      "  profiler: null\n",
      "  benchmark: false\n",
      "  deterministic: false\n",
      "  reload_dataloaders_every_n_epochs: 0\n",
      "  auto_lr_find: false\n",
      "  replace_sampler_ddp: true\n",
      "  detect_anomaly: false\n",
      "  auto_scale_batch_size: false\n",
      "  plugins: null\n",
      "  move_metrics_to_cpu: false\n",
      "  multiple_trainloader_mode: max_size_cycle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/students/tyang/anomalib/src/anomalib/config/config.py:243: UserWarning: The seed value is now fixed to 0. Up to v0.3.7, the seed was not fixed when the seed value was set to 0. If you want to use the random seed, please select `None` for the seed value (`null` in the YAML file) or remove the `seed` key from the YAML file.\n",
      "  warn(\n",
      "/home/students/tyang/anomalib/src/anomalib/config/config.py:280: UserWarning: config.project.unique_dir is set to False. This does not ensure that your results will be written in an empty directory and you may overwrite files.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "CONFIG_select_PATH = f\"/home/students/tyang/anomalib/src/anomalib/models/{MODEL}/config_select.yaml\"\n",
    "with open(file=CONFIG_select_PATH, mode=\"r\",encoding=\"utf-8\") as f:\n",
    "    print(f.read())\n",
    "    \n",
    "config_select = get_configurable_parameters(config_path=CONFIG_select_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/students/tyang/anomalib/src/anomalib/data/utils/split.py:110: UserWarning: Zero subset length encountered during splitting. This means one of your subsets might be empty or devoid of either normal or anomalous images.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "select_data_module = get_datamodule(config=config_select)\n",
    "select_data_module.prepare_data() # check if the dataset is avaliable\n",
    "select_data_module.setup()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "similiar_data_module = get_datamodule(config=config_similiar)\n",
    "similiar_data_module.prepare_data() # check if the dataset is avaliable\n",
    "similiar_data_module.setup()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "invTrans = torchvision.transforms.Compose([ torchvision.transforms.Normalize(mean = [ 0., 0., 0. ],\n",
    "                                                                            std = [ 1/0.229, 1/0.224, 1/0.225 ]),\n",
    "                                            torchvision.transforms.Normalize(mean = [ -0.485, -0.456, -0.406 ],\n",
    "                                                                            std = [ 1., 1., 1. ]),\n",
    "                                            torchvision.transforms.Resize((640,640)),\n",
    "                                            \n",
    "                                            ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "i, train_data = next(enumerate(similiar_data_module.train_dataloader()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/scratch/tyang/miniconda3/env/anomalib_env/lib/python3.10/site-packages/torch/overrides.py:110: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  torch.has_cuda,\n",
      "/work/scratch/tyang/miniconda3/env/anomalib_env/lib/python3.10/site-packages/torch/overrides.py:111: UserWarning: 'has_cudnn' is deprecated, please use 'torch.backends.cudnn.is_available()'\n",
      "  torch.has_cudnn,\n",
      "/work/scratch/tyang/miniconda3/env/anomalib_env/lib/python3.10/site-packages/torch/overrides.py:117: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  torch.has_mps,\n",
      "/work/scratch/tyang/miniconda3/env/anomalib_env/lib/python3.10/site-packages/torch/overrides.py:118: UserWarning: 'has_mkldnn' is deprecated, please use 'torch.backends.mkldnn.is_available()'\n",
      "  torch.has_mkldnn,\n"
     ]
    }
   ],
   "source": [
    "\n",
    "feature_extractor = TorchFXFeatureExtractor(\n",
    "                    backbone=\"densenet201\",\n",
    "                    return_nodes=[\"features.denseblock1.denselayer6.conv2\"],\n",
    "                    weights=DenseNet201_Weights.IMAGENET1K_V1,\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 32, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "# feature shape is (batch_size, channel, height, width) \n",
    "feature = feature_extractor(train_data[\"image\"])\n",
    "print(feature[\"features.denseblock1.denselayer6.conv2\"].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000])\n"
     ]
    }
   ],
   "source": [
    "print(train_data[\"label\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract features from the whole training dataset\n",
    "\n",
    "feature_list = []\n",
    "\n",
    "for  i, train_data in enumerate(similiar_data_module.train_dataloader()):\n",
    "    features = feature_extractor(train_data[\"image\"])[\"features.denseblock1.denselayer6.conv2\"]\n",
    "    feature_list.append(features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the list of features to a tensor\n",
    "global_feature_tensor = torch.vstack(feature_list)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([15658, 32, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "# global feature shape is (train_data size, channel, height, width)\n",
    "print(global_feature_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anomalib.models.components.cluster.kmeans import KMeans\n",
    "\n",
    "def get_kmeans_centers(feature_tensor, n_clusters):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        feature_t: feature tensor, shape is (batch_size, channel, height, width)\n",
    "        n_clusters: number of clusters\n",
    "        \n",
    "    Returns:\n",
    "        cluster_center: shape is (n_clusters, channel)\n",
    "        kmeans: kmeans model   \"\"\"\n",
    "    \n",
    "    feature_t = feature_tensor.permute(1,0,2,3)\n",
    "    feature_t = feature_t.flatten(start_dim=1)\n",
    "    feature_t= feature_t.permute(1,0)\n",
    "\n",
    "\n",
    "    kmeans = KMeans(n_clusters=n_clusters)\n",
    "    kmeans.fit(feature_t)\n",
    "    cluster_center = kmeans.cluster_centers_\n",
    "   \n",
    "    return cluster_center, kmeans\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 32])\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "clusters_centers,kmeans = get_kmeans_centers(global_feature_tensor, n_clusters=2)\n",
    "\n",
    "print(clusters_centers.shape)\n",
    "print(len(clusters_centers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words_statistics(Ptst, Cref, S):\n",
    "    \"\"\"\n",
    "    Params:\n",
    "    Ptst: Feature tensor of a set of images, tensor of shape (N, C, H, W)\n",
    "    Cref: reference Cluster centers, tensor of shape (K, C)\n",
    "    S: number of subregions per image dimension, integer\n",
    "\n",
    "    Returns:\n",
    "    bow_stats: list of normalized Bag-of-words statistics, possibility-like , length N, each element is a tensor of shape (S * S, K)\n",
    "    \"\"\"\n",
    "    Ptst = torch.vsplit(Ptst, Ptst.shape[0])\n",
    "    bow_stats = []\n",
    "    for Itst in Ptst:\n",
    "        Itst = Itst.squeeze(0)\n",
    "        #print(Itst.shape)\n",
    "        subtensors = torch.chunk(Itst, S, dim=1)\n",
    "        subtensor = [torch.chunk(st, S, dim=2) for st in subtensors]\n",
    "        \n",
    "        \n",
    "        image_bow_stats = torch.zeros(S * S, len(Cref), dtype=torch.float32)\n",
    "        for i in range(S):\n",
    "            for j in range(S):\n",
    "                st_value = subtensor[i][j]\n",
    "                st_value = st_value.flatten(start_dim=1)\n",
    "                st_value = st_value.permute(1,0)\n",
    "               # print(st_value.shape)\n",
    "                \n",
    "                cluster_idx = kmeans.predict(st_value)\n",
    "                #print(cluster_idx.shape)\n",
    "                cluster_idx = cluster_idx.float()\n",
    "\n",
    "                hist = torch.histc(cluster_idx, bins = len(Cref), min = torch.min(cluster_idx), max = torch.max(cluster_idx))\n",
    "                normalized_hist = hist / torch.sum(hist)\n",
    "                image_bow_stats[i * S + j] = normalized_hist\n",
    "        \n",
    "        bow_stats.append(image_bow_stats)\n",
    "            \n",
    "        \n",
    "    return bow_stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_bow_stats = bag_of_words_statistics(global_feature_tensor, clusters_centers, S=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/students/tyang/yolov5results/cat0/crops/od/TRAIN000076.jpg']\n",
      "['/home/students/tyang/yolov5results/cat0/crops/od/TRAIN002718.jpg']\n",
      "['/home/students/tyang/yolov5results/cat0/crops/od/TRAIN003406.jpg']\n",
      "['/home/students/tyang/yolov5results/cat0/crops/od/TRAIN003539.jpg']\n",
      "['/home/students/tyang/yolov5results/cat0/crops/od/TRAIN005134.jpg']\n",
      "['/home/students/tyang/yolov5results/cat0/crops/od/TRAIN008303.jpg']\n",
      "['/home/students/tyang/yolov5results/cat0/crops/od/TRAIN010313.jpg']\n",
      "['/home/students/tyang/yolov5results/cat0/crops/od/TRAIN010730.jpg']\n",
      "['/home/students/tyang/yolov5results/cat0/crops/od/TRAIN017815.jpg']\n"
     ]
    }
   ],
   "source": [
    "for i, train_data in enumerate(select_data_module.train_dataloader()):\n",
    "    \n",
    "    print(train_data[\"image_path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'select_data_module' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m,\u001b[38;5;241m10\u001b[39m))\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, select_data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mselect_data_module\u001b[49m\u001b[38;5;241m.\u001b[39mtrain_dataloader()):\n\u001b[1;32m      6\u001b[0m     plt\u001b[38;5;241m.\u001b[39msubplot(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m, i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      7\u001b[0m     select_image \u001b[38;5;241m=\u001b[39m select_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'select_data_module' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "for i, select_data in enumerate(select_data_module.train_dataloader()):\n",
    "    plt.subplot(3, 3, i+1)\n",
    "    select_image = select_data[\"image\"]\n",
    "    conv_img = invTrans(select_image)\n",
    "    plt.imshow(TF.to_pil_image(conv_img.squeeze()))\n",
    "    plt.axis('off')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_feature_list = []\n",
    "\n",
    "for  i, train_data in enumerate(select_data_module.train_dataloader()):\n",
    "    features = feature_extractor(train_data[\"image\"])[\"features.denseblock1.denselayer6.conv2\"]\n",
    "    reference_feature_list.append(features)\n",
    "\n",
    "reference_feature_tensor = torch.vstack(reference_feature_list) \n",
    "reference_bow_stats = bag_of_words_statistics(reference_feature_tensor, clusters_centers, S=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(global_bow_stats, \"/home/students/tyang/Documents/global_bow_stats.pt\")\n",
    "torch.save(reference_bow_stats, \"/home/students/tyang/Documents/reference_bow_stats.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(global_bow_stats))\n",
    "print(global_bow_stats[0].shape)\n",
    "print(len(reference_bow_stats))\n",
    "print(reference_bow_stats[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_similiarity(bow_stats, bow_stats_ref, select_ratio, step_size):\n",
    "    \"\"\"\n",
    "    Params:\n",
    "    bow_stats: list of normalized Bag-of-words statistics, possibility-like , length N, each element is a tensor of shape (S * S, K)\n",
    "    bow_stats_ref:  normalized Bag-of-words statistics of reference image, \n",
    "    possibility-like , length N, each element is a tensor of shape (S * S, K)\n",
    "    step_size: step size of sliding window, integer, should be the same as batchsize\n",
    "\n",
    "    Returns:\n",
    "    similiarity: list of similiarity, length N, each element is a tensor of shape (S * S, S * S)\n",
    "    \"\"\"\n",
    "    stacked_bow_stats = torch.stack(bow_stats) # shape is (N, S * S, K)\n",
    "    #print(stacked_bow_stats.shape)\n",
    "    bow_stats_ref = bow_stats_ref.unsqueeze(0)  # shape is (1, S * S, K)\n",
    "   # print(bow_stats_ref.shape)\n",
    "\n",
    "    stacked_bow_stats[stacked_bow_stats == 0 ] = 1e-10\n",
    "    bow_stats_ref[bow_stats_ref == 0 ] = 1e-10\n",
    "\n",
    "    selected_idxs =[]\n",
    "    selected_distances = []\n",
    "    for i in range(0, stacked_bow_stats.shape[0], step_size):\n",
    "        current_bow = stacked_bow_stats[i:i+step_size if i+step_size < stacked_bow_stats.shape[0] else stacked_bow_stats.shape[0], :, :]\n",
    "        expended_bow_stats_ref = bow_stats_ref.expand(current_bow.shape[0], bow_stats_ref.shape[1], bow_stats_ref.shape[2])\n",
    "\n",
    "        kl_divegence = F.kl_div(current_bow.log(), expended_bow_stats_ref, reduction=\"none\")\n",
    "       \n",
    "        kl_total = torch.sum(kl_divegence, dim=2)\n",
    "        distance = torch.sum(kl_total, dim=1)\n",
    "        topk_near_distances,near_idx = torch.topk(distance, k=int(len(distance) * select_ratio), dim=0, largest=False, sorted=True)\n",
    "        \n",
    "        selected_idxs.append(near_idx)\n",
    "        \n",
    "        selected_distances.append(topk_near_distances)\n",
    "\n",
    "    return selected_idxs, selected_distances\n",
    "\n",
    "        \n",
    "       \n",
    "\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref0_idxs,distances = compute_similiarity(global_bow_stats, reference_bow_stats[0], select_ratio=0.05, step_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def second_selection(index, distance, select_ratio):\n",
    "    \"\"\"\n",
    "    Params:\n",
    "    idxs: list of selected image indices in each batch , length number of batches\n",
    "    distances: list of selected image distances, length number of batches\n",
    "    select_ratio: ratio of selected images compare to all images, float\n",
    "\n",
    "    Returns:\n",
    "    selected_idx: list of selected image indices with biggest distances over all in each batches, length number of batches\n",
    "    \"\"\"\n",
    "    selected_distances = [[] for _ in range(len(distance))]\n",
    "    distance_t = torch.cat(distance)\n",
    "    topk, global_ids = torch.topk(distance_t, k=int(len(distance_t) * select_ratio ), dim=0, largest=False, sorted=True)\n",
    "    selected_idxs = [[] for _ in range(len(index))]\n",
    "    \n",
    "    for global_id in global_ids:\n",
    "        batch_id = global_id // index[0].shape[0]\n",
    "        local_id = global_id % index[0].shape[0]\n",
    "        selected_idxs[batch_id].append(index[batch_id][local_id])\n",
    "        selected_distances[batch_id].append(distance_t[global_id])\n",
    "        \n",
    "  \n",
    "    \n",
    "    return selected_idxs, selected_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_indices,second_dists = second_selection(ref0_idxs, distances, select_ratio=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(second_indices))\n",
    "print(len(second_dists))\n",
    "print(second_indices[0])\n",
    "print(len(second_indices[0]))\n",
    "print(len(second_indices[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_datas = {\"image_path\": [], \"label\": []}\n",
    "for  i, train_data in enumerate(similiar_data_module.train_dataloader()):\n",
    "    selected_data = ( {\"image_path\": train_data[\"image_path\"][second_indice], \"label\": train_data[\"label\"][second_indice]} for second_indice in second_indices[i] )\n",
    "    \n",
    "    \n",
    "    for data in selected_data:\n",
    "        for key, values in data.items():\n",
    "            merged_datas[key].append(values)\n",
    "    \n",
    "\n",
    "print(merged_datas[\"image_path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(merged_datas[\"image_path\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv \n",
    "\n",
    "csv_path = \"/home/students/tyang/Documents/similiar_csv_files/category0_ref0.csv\"\n",
    "\n",
    "\n",
    "with open(csv_path, mode=\"w\", newline=\"\") as csv_file:\n",
    "        fieldnames = [\"image_path\", \"label\"]\n",
    "        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "        for i in range(len(merged_datas[\"image_path\"])):\n",
    "            rowdict = {\"image_path\": merged_datas[\"image_path\"][i], \"label\": merged_datas[\"label\"][i]}\n",
    "            writer.writerow(rowdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref1_idxs,distances = compute_similiarity(global_bow_stats, reference_bow_stats[1], select_ratio=0.05, step_size=1000)\n",
    "second_indices,second_dists = second_selection(ref1_idxs, distances, select_ratio=0.9)\n",
    "\n",
    "merged_datas = {\"image_path\": [], \"label\": []}\n",
    "for  i, train_data in enumerate(similiar_data_module.train_dataloader()):\n",
    "    selected_data = ( {\"image_path\": train_data[\"image_path\"][second_indice], \"label\": train_data[\"label\"][second_indice]} for second_indice in second_indices[i] )\n",
    "    \n",
    "    \n",
    "    for data in selected_data:\n",
    "        for key, values in data.items():\n",
    "            merged_datas[key].append(values)\n",
    "    \n",
    "\n",
    "import csv \n",
    "\n",
    "csv_path = \"/home/students/tyang/Documents/similiar_csv_files/category0_ref1.csv\"\n",
    "\n",
    "\n",
    "with open(csv_path, mode=\"w\", newline=\"\") as csv_file:\n",
    "        fieldnames = [\"image_path\", \"label\"]\n",
    "        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "        for i in range(len(merged_datas[\"image_path\"])):\n",
    "            rowdict = {\"image_path\": merged_datas[\"image_path\"][i], \"label\": merged_datas[\"label\"][i]}\n",
    "            writer.writerow(rowdict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref2_idxs,distances = compute_similiarity(global_bow_stats, reference_bow_stats[2], select_ratio=0.05, step_size=1000)\n",
    "second_indices,second_dists = second_selection(ref2_idxs, distances, select_ratio=0.9)\n",
    "\n",
    "merged_datas = {\"image_path\": [], \"label\": []}\n",
    "for  i, train_data in enumerate(similiar_data_module.train_dataloader()):\n",
    "    selected_data = ( {\"image_path\": train_data[\"image_path\"][second_indice], \"label\": train_data[\"label\"][second_indice]} for second_indice in second_indices[i] )\n",
    "    \n",
    "    \n",
    "    for data in selected_data:\n",
    "        for key, values in data.items():\n",
    "            merged_datas[key].append(values)\n",
    "    \n",
    "\n",
    "\n",
    "csv_path = \"/home/students/tyang/Documents/similiar_csv_files/category0_ref2.csv\"\n",
    "\n",
    "\n",
    "with open(csv_path, mode=\"w\", newline=\"\") as csv_file:\n",
    "        fieldnames = [\"image_path\", \"label\"]\n",
    "        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "        for i in range(len(merged_datas[\"image_path\"])):\n",
    "            rowdict = {\"image_path\": merged_datas[\"image_path\"][i], \"label\": merged_datas[\"label\"][i]}\n",
    "            writer.writerow(rowdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "ref3_idxs,distances = compute_similiarity(global_bow_stats, reference_bow_stats[3], select_ratio=0.05, step_size=1000)\n",
    "second_indices,second_dists = second_selection(ref3_idxs, distances, select_ratio=0.9)\n",
    "\n",
    "merged_datas = {\"image_path\": [], \"label\": []}\n",
    "for  i, train_data in enumerate(similiar_data_module.train_dataloader()):\n",
    "    selected_data = ( {\"image_path\": train_data[\"image_path\"][second_indice], \"label\": train_data[\"label\"][second_indice]} for second_indice in second_indices[i] )\n",
    "    \n",
    "    \n",
    "    for data in selected_data:\n",
    "        for key, values in data.items():\n",
    "            merged_datas[key].append(values)\n",
    "    \n",
    "\n",
    "\n",
    "csv_path = \"/home/students/tyang/Documents/similiar_csv_files/category0_ref3.csv\"\n",
    "\n",
    "\n",
    "with open(csv_path, mode=\"w\", newline=\"\") as csv_file:\n",
    "        fieldnames = [\"image_path\", \"label\"]\n",
    "        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "        for i in range(len(merged_datas[\"image_path\"])):\n",
    "            rowdict = {\"image_path\": merged_datas[\"image_path\"][i], \"label\": merged_datas[\"label\"][i]}\n",
    "            writer.writerow(rowdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref4_idxs,distances = compute_similiarity(global_bow_stats, reference_bow_stats[4], select_ratio=0.05, step_size=1000)\n",
    "second_indices,second_dists = second_selection(ref4_idxs, distances, select_ratio=0.9)\n",
    "\n",
    "merged_datas = {\"image_path\": [], \"label\": []}\n",
    "for  i, train_data in enumerate(similiar_data_module.train_dataloader()):\n",
    "    selected_data = ( {\"image_path\": train_data[\"image_path\"][second_indice], \"label\": train_data[\"label\"][second_indice]} for second_indice in second_indices[i] )\n",
    "    \n",
    "    \n",
    "    for data in selected_data:\n",
    "        for key, values in data.items():\n",
    "            merged_datas[key].append(values)\n",
    "    \n",
    "\n",
    "\n",
    "csv_path = \"/home/students/tyang/Documents/similiar_csv_files/category0_ref4.csv\"\n",
    "\n",
    "\n",
    "with open(csv_path, mode=\"w\", newline=\"\") as csv_file:\n",
    "        fieldnames = [\"image_path\", \"label\"]\n",
    "        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "        for i in range(len(merged_datas[\"image_path\"])):\n",
    "            rowdict = {\"image_path\": merged_datas[\"image_path\"][i], \"label\": merged_datas[\"label\"][i]}\n",
    "            writer.writerow(rowdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref5_idxs,distances = compute_similiarity(global_bow_stats, reference_bow_stats[5], select_ratio=0.05, step_size=1000)\n",
    "second_indices,second_dists = second_selection(ref5_idxs, distances, select_ratio=0.9)\n",
    "\n",
    "merged_datas = {\"image_path\": [], \"label\": []}\n",
    "for  i, train_data in enumerate(similiar_data_module.train_dataloader()):\n",
    "    selected_data = ( {\"image_path\": train_data[\"image_path\"][second_indice], \"label\": train_data[\"label\"][second_indice]} for second_indice in second_indices[i] )\n",
    "    \n",
    "    \n",
    "    for data in selected_data:\n",
    "        for key, values in data.items():\n",
    "            merged_datas[key].append(values)\n",
    "    \n",
    "\n",
    "\n",
    "csv_path = \"/home/students/tyang/Documents/similiar_csv_files/category0_ref5.csv\"\n",
    "\n",
    "\n",
    "with open(csv_path, mode=\"w\", newline=\"\") as csv_file:\n",
    "        fieldnames = [\"image_path\", \"label\"]\n",
    "        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "        for i in range(len(merged_datas[\"image_path\"])):\n",
    "            rowdict = {\"image_path\": merged_datas[\"image_path\"][i], \"label\": merged_datas[\"label\"][i]}\n",
    "            writer.writerow(rowdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref6_idxs,distances = compute_similiarity(global_bow_stats, reference_bow_stats[6], select_ratio=0.05, step_size=1000)\n",
    "second_indices,second_dists = second_selection(ref6_idxs, distances, select_ratio=0.9)\n",
    "\n",
    "merged_datas = {\"image_path\": [], \"label\": []}\n",
    "for  i, train_data in enumerate(similiar_data_module.train_dataloader()):\n",
    "    selected_data = ( {\"image_path\": train_data[\"image_path\"][second_indice], \"label\": train_data[\"label\"][second_indice]} for second_indice in second_indices[i] )\n",
    "    \n",
    "    \n",
    "    for data in selected_data:\n",
    "        for key, values in data.items():\n",
    "            merged_datas[key].append(values)\n",
    "    \n",
    "\n",
    "\n",
    "csv_path = \"/home/students/tyang/Documents/similiar_csv_files/category0_ref6.csv\"\n",
    "\n",
    "\n",
    "with open(csv_path, mode=\"w\", newline=\"\") as csv_file:\n",
    "        fieldnames = [\"image_path\", \"label\"]\n",
    "        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "        for i in range(len(merged_datas[\"image_path\"])):\n",
    "            rowdict = {\"image_path\": merged_datas[\"image_path\"][i], \"label\": merged_datas[\"label\"][i]}\n",
    "            writer.writerow(rowdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref7_idxs,distances = compute_similiarity(global_bow_stats, reference_bow_stats[7], select_ratio=0.05, step_size=1000)\n",
    "second_indices,second_dists = second_selection(ref7_idxs, distances, select_ratio=0.9)\n",
    "\n",
    "merged_datas = {\"image_path\": [], \"label\": []}\n",
    "for  i, train_data in enumerate(similiar_data_module.train_dataloader()):\n",
    "    selected_data = ( {\"image_path\": train_data[\"image_path\"][second_indice], \"label\": train_data[\"label\"][second_indice]} for second_indice in second_indices[i] )\n",
    "    \n",
    "    \n",
    "    for data in selected_data:\n",
    "        for key, values in data.items():\n",
    "            merged_datas[key].append(values)\n",
    "    \n",
    "\n",
    "\n",
    "csv_path = \"/home/students/tyang/Documents/similiar_csv_files/category0_ref7.csv\"\n",
    "\n",
    "\n",
    "with open(csv_path, mode=\"w\", newline=\"\") as csv_file:\n",
    "        fieldnames = [\"image_path\", \"label\"]\n",
    "        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "        for i in range(len(merged_datas[\"image_path\"])):\n",
    "            rowdict = {\"image_path\": merged_datas[\"image_path\"][i], \"label\": merged_datas[\"label\"][i]}\n",
    "            writer.writerow(rowdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "ref8_idxs,distances = compute_similiarity(global_bow_stats, reference_bow_stats[8], select_ratio=0.05, step_size=1000)\n",
    "second_indices,second_dists = second_selection(ref8_idxs, distances, select_ratio=0.9)\n",
    "\n",
    "merged_datas = {\"image_path\": [], \"label\": []}\n",
    "for  i, train_data in enumerate(similiar_data_module.train_dataloader()):\n",
    "    selected_data = ( {\"image_path\": train_data[\"image_path\"][second_indice], \"label\": train_data[\"label\"][second_indice]} for second_indice in second_indices[i] )\n",
    "    \n",
    "    \n",
    "    for data in selected_data:\n",
    "        for key, values in data.items():\n",
    "            merged_datas[key].append(values)\n",
    "    \n",
    "\n",
    "\n",
    "csv_path = \"/home/students/tyang/Documents/similiar_csv_files/category0_ref8.csv\"\n",
    "\n",
    "\n",
    "with open(csv_path, mode=\"w\", newline=\"\") as csv_file:\n",
    "        fieldnames = [\"image_path\", \"label\"]\n",
    "        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "        for i in range(len(merged_datas[\"image_path\"])):\n",
    "            rowdict = {\"image_path\": merged_datas[\"image_path\"][i], \"label\": merged_datas[\"label\"][i]}\n",
    "            writer.writerow(rowdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"/home/students/tyang/Documents/kmeans_category_0_cpr.pkl\", \"wb\") as f:\n",
    "    pickle.dump(kmeans, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(feature_extractor, \"/home/students/tyang/Documents/feature_extractor_category_0_cpr.pt\")\n",
    "torch.save(clusters_centers, \"/home/students/tyang/Documents/cluster_centers_category_0_cpr.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(kmeans, \"/home/students/tyang/Documents/kmeans_category_0_cpr.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
