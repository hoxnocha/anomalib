{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/scratch/tyang/miniconda3/env/anomalib_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "OpenVINO is not installed. Please install OpenVINO to use OpenVINOInferencer.\n",
      "OpenVINO is not installed. Please install OpenVINO to use OpenVINOInferencer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To use wandb logger install it using `pip install wandb`\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import shutil\n",
    "import tempfile\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "from typing import Tuple\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from anomalib.config import get_configurable_parameters\n",
    "from anomalib.data import get_datamodule\n",
    "from anomalib.data.inference import InferenceDataset\n",
    "from anomalib.models import get_model\n",
    "from anomalib.models.patchcore.lightning_model import PatchcoreLightning\n",
    "from anomalib.post_processing.post_process import compute_mask\n",
    "from anomalib.utils.callbacks import get_callbacks\n",
    "from anomalib.utils.loggers import get_experiment_logger\n",
    "from pytorch_lightning import Trainer, seed_everything\n",
    "from skimage.morphology import dilation\n",
    "from skimage.segmentation import find_boundaries\n",
    "from torch.utils.data import DataLoader\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = Path(\"/home/students/tyang/Downloads\")\n",
    "IDRID_ZIP_LOCATION = DATA_FOLDER / \"B. Disease Grading.zip\"\n",
    "\n",
    "PATCHCORE_CONFIG_PATH = \"/home/students/tyang/anomalib/src/anomalib/models/patchcore/idrid.yaml\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tempfile.TemporaryDirectory() as tmp_data_dir:\n",
    "    with zipfile.ZipFile(IDRID_ZIP_LOCATION, \"r\") as idrid_zip_ref:\n",
    "        idrid_zip_ref.extractall(tmp_data_dir)\n",
    "\n",
    "        tmp_data_dir = Path(tmp_data_dir)\n",
    "        gt_path = tmp_data_dir / \"B. Disease Grading\" / \"2. Groundtruths\"\n",
    "        original_img_path = tmp_data_dir / \"B. Disease Grading\" / \"1. Original Images\"\n",
    "\n",
    "        normal_img_dst = DATA_FOLDER / \"idrid\" / \"normal\"\n",
    "        normal_img_dst.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "        dr_img_dst = DATA_FOLDER / \"idrid\" / \"dr\"\n",
    "        dr_img_dst.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "        for csv_fn, dir_name in [\n",
    "            (\"a. IDRiD_Disease Grading_Training Labels.csv\", \"a. Training Set\"),\n",
    "            (\"b. IDRiD_Disease Grading_Testing Labels.csv\", \"b. Testing Set\"),\n",
    "        ]:\n",
    "            gt_csv = pd.read_csv(gt_path / csv_fn)\n",
    "\n",
    "            no_retinopathy = gt_csv[gt_csv[\"Retinopathy grade\"] == 0]\n",
    "            for _, no_retinopathy_img in no_retinopathy.iterrows():\n",
    "                shutil.copy(\n",
    "                    original_img_path\n",
    "                    / dir_name\n",
    "                    / (no_retinopathy_img[\"Image name\"] + \".jpg\"),\n",
    "                    normal_img_dst\n",
    "                    / f\"{dir_name.strip()}_{no_retinopathy_img['Image name']}.jpg\",\n",
    "                )\n",
    "\n",
    "            for retinopathy_grade in range(1, 5):\n",
    "                cur_dst = dr_img_dst / str(retinopathy_grade)\n",
    "                cur_dst.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "                retinopathy_subset = gt_csv[\n",
    "                    gt_csv[\"Retinopathy grade\"] == retinopathy_grade\n",
    "                ]\n",
    "                for _, retinopathy_img in retinopathy_subset.iterrows():\n",
    "                    shutil.copy(\n",
    "                        original_img_path\n",
    "                        / dir_name\n",
    "                        / (retinopathy_img[\"Image name\"] + \".jpg\"),\n",
    "                        cur_dst\n",
    "                        / f\"{dir_name.strip()}_{retinopathy_img['Image name']}.jpg\",\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/students/tyang/anomalib/src/anomalib/config/config.py:243: UserWarning: The seed value is now fixed to 0. Up to v0.3.7, the seed was not fixed when the seed value was set to 0. If you want to use the random seed, please select `None` for the seed value (`null` in the YAML file) or remove the `seed` key from the YAML file.\n",
      "  warn(\n",
      "/home/students/tyang/anomalib/src/anomalib/config/config.py:280: UserWarning: config.project.unique_dir is set to False. This does not ensure that your results will be written in an empty directory and you may overwrite files.\n",
      "  warn(\n",
      "/work/scratch/tyang/miniconda3/env/anomalib_env/lib/python3.10/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `PrecisionRecallCurve` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model wide_resnet50_2 from torchvision\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/scratch/tyang/miniconda3/env/anomalib_env/lib/python3.10/site-packages/torch/overrides.py:110: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  torch.has_cuda,\n",
      "/work/scratch/tyang/miniconda3/env/anomalib_env/lib/python3.10/site-packages/torch/overrides.py:111: UserWarning: 'has_cudnn' is deprecated, please use 'torch.backends.cudnn.is_available()'\n",
      "  torch.has_cudnn,\n",
      "/work/scratch/tyang/miniconda3/env/anomalib_env/lib/python3.10/site-packages/torch/overrides.py:117: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  torch.has_mps,\n",
      "/work/scratch/tyang/miniconda3/env/anomalib_env/lib/python3.10/site-packages/torch/overrides.py:118: UserWarning: 'has_mkldnn' is deprecated, please use 'torch.backends.mkldnn.is_available()'\n",
      "  torch.has_mkldnn,\n",
      "/home/students/tyang/anomalib/src/anomalib/utils/callbacks/__init__.py:153: UserWarning: Export option: None not found. Defaulting to no model export\n",
      "  warnings.warn(f\"Export option: {config.optimization.export_mode} not found. Defaulting to no model export\")\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(limit_train_batches=1.0)` was configured so 100% of the batches per epoch will be used..\n",
      "`Trainer(limit_val_batches=1.0)` was configured so 100% of the batches will be used..\n",
      "`Trainer(limit_test_batches=1.0)` was configured so 100% of the batches will be used..\n",
      "`Trainer(limit_predict_batches=1.0)` was configured so 100% of the batches will be used..\n",
      "`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n",
      "/work/scratch/tyang/miniconda3/env/anomalib_env/lib/python3.10/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Metric `ROC` will save all targets and predictions in buffer. For large datasets this may lead to large memory footprint.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/work/scratch/tyang/miniconda3/env/anomalib_env/lib/python3.10/site-packages/pytorch_lightning/core/optimizer.py:183: UserWarning: `LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name                  | Type                     | Params\n",
      "-------------------------------------------------------------------\n",
      "0 | image_threshold       | AnomalyScoreThreshold    | 0     \n",
      "1 | pixel_threshold       | AnomalyScoreThreshold    | 0     \n",
      "2 | model                 | PatchcoreModel           | 24.9 M\n",
      "3 | image_metrics         | AnomalibMetricCollection | 0     \n",
      "4 | pixel_metrics         | AnomalibMetricCollection | 0     \n",
      "5 | normalization_metrics | MinMax                   | 0     \n",
      "-------------------------------------------------------------------\n",
      "24.9 M    Trainable params\n",
      "0         Non-trainable params\n",
      "24.9 M    Total params\n",
      "99.450    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  50%|█████     | 3/6 [00:08<00:08,  2.98s/it, loss=nan]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/scratch/tyang/miniconda3/env/anomalib_env/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py:138: UserWarning: `training_step` returned `None`. If this was on purpose, ignore this warning...\n",
      "  self.warning_cache.warn(\"`training_step` returned `None`. If this was on purpose, ignore this warning...\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  83%|████████▎ | 5/6 [00:09<00:01,  1.84s/it, loss=nan]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "Selecting Coreset Indices.: 100%|██████████| 13824/13824 [00:40<00:00, 344.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 6/6 [00:55<00:00,  9.21s/it, loss=nan, pixel_F1Score=0.109, pixel_AUROC=0.807]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 6/6 [01:13<00:00, 12.17s/it, loss=nan, pixel_F1Score=0.109, pixel_AUROC=0.807]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "config = get_configurable_parameters(\n",
    "    model_name=\"patchcore\", config_path=PATCHCORE_CONFIG_PATH\n",
    ")\n",
    "if config.project.seed:\n",
    "    seed_everything(config.project.seed)\n",
    "\n",
    "datamodule = get_datamodule(config)\n",
    "model = get_model(config)\n",
    "experiment_logger = get_experiment_logger(config)\n",
    "callbacks = get_callbacks(config)\n",
    "     \n",
    "\n",
    "trainer = Trainer(**config.trainer, logger=experiment_logger, callbacks=callbacks)\n",
    "trainer.fit(model=model, datamodule=datamodule)\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_predictor(\n",
    "    model_config_path: str, model_weight_path: str\n",
    ") -> Tuple[PatchcoreLightning, Trainer]:\n",
    "    \"\"\"Load anomalib model\"\"\"\n",
    "    pred_config = get_configurable_parameters(config_path=model_config_path)\n",
    "    pred_config.trainer.resume_from_checkpoint = model_weight_path\n",
    "    pred_config.visualization.show_images = False\n",
    "    pred_config.visualization.save_images = False\n",
    "\n",
    "    model = get_model(pred_config)\n",
    "    pred_callbacks = get_callbacks(pred_config)\n",
    "\n",
    "    pred_trainer = Trainer(callbacks=pred_callbacks, **pred_config.trainer)\n",
    "    return model, pred_trainer\n",
    "\n",
    "\n",
    "def get_prediction(\n",
    "    img_path: str,\n",
    "    model: PatchcoreLightning,\n",
    "    pred_trainer: Trainer,\n",
    "    img_size: Tuple[int, int] = (256, 256),\n",
    "):\n",
    "    \"\"\"Get a prediction for an image at img_path by a model with given config\"\"\"\n",
    "\n",
    "    dataset = InferenceDataset(\n",
    "        img_path,\n",
    "        image_size=img_size,\n",
    "        transform=None,\n",
    "    )\n",
    "    dataloader = DataLoader(dataset)\n",
    "    return pred_trainer.predict(model=model, dataloaders=[dataloader])\n",
    "\n",
    "\n",
    "def add_segmentation_boundary(\n",
    "    image: np.ndarray,\n",
    "    anomaly_map: np.ndarray,\n",
    "    threshold: float = 0.5,\n",
    "    thickness: int = 20,\n",
    "    color: Tuple[int, int, int] = (255, 0, 0),\n",
    "):\n",
    "    \"\"\"Add a segmentation boundary based on an anomaly map\"\"\"\n",
    "    marked = np.copy(image)\n",
    "    anomaly_mask = compute_mask(anomaly_map, threshold)\n",
    "    boundaries = find_boundaries(anomaly_mask)\n",
    "    outlines = dilation(boundaries, np.ones((thickness, thickness)))\n",
    "    marked[outlines] = color\n",
    "    return marked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_weight_path = (\n",
    "    \"/home/students/tyang/anomalib/results/patchcore/idrid/run/weights/lightning/model.ckpt\"\n",
    "\n",
    ")\n",
    "model, pred_trainer = prepare_predictor(PATCHCORE_CONFIG_PATH, model_weight_path)\n",
    "\n",
    "patchcore_results = collections.defaultdict(dict)\n",
    "for grade in range(1, 5):\n",
    "    img_folder = (\n",
    "        DATA_FOLDER / \"idrid\" / \"dr\" / str(grade)\n",
    "        if grade > 0\n",
    "        else DATA_FOLDER / \"idrid\" / \"normal\"\n",
    "    )\n",
    "    img_paths = sorted((img_folder).glob(\"*.jpg\"))[:4]\n",
    "    for img_path in img_paths:\n",
    "        patchcore_results[grade][img_path] = get_prediction(\n",
    "            img_path, model, pred_trainer\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for grade in range(1, 5):\n",
    "    grade_results = patchcore_results[grade]\n",
    "\n",
    "    plt.figure(figsize=(10, 15))\n",
    "    for img_idx, img_fn in enumerate(grade_results):\n",
    "        img = cv2.imread(img_fn.as_posix())[:, :, ::-1]  # BGR RGB conversion\n",
    "\n",
    "        anomaly_map = np.squeeze(grade_results[img_fn][0][\"anomaly_maps\"].numpy())\n",
    "        anomaly_map_original_size = cv2.resize(\n",
    "            anomaly_map, (img.shape[1], img.shape[0])\n",
    "        )\n",
    "        segmentation_boundary = add_segmentation_boundary(\n",
    "            img, anomaly_map_original_size, threshold=0.3\n",
    "        )\n",
    "\n",
    "        plt.subplot(len(grade_results), 3, img_idx * 3 + 1)\n",
    "        plt.imshow(img)\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        plt.subplot(len(grade_results), 3, img_idx * 3 + 2)\n",
    "        plt.imshow(img)\n",
    "        plt.imshow(anomaly_map_original_size, alpha=0.5)\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "        plt.subplot(len(grade_results), 3, img_idx * 3 + 3)\n",
    "        plt.imshow(segmentation_boundary)\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "    plt.subplots_adjust(wspace=0.05, hspace=-0.7)\n",
    "    plt.suptitle(f\"PatchCore results for DR grade {grade}\", y=0.75)\n",
    "    plt.show()\n",
    "     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
