{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/scratch/tyang/miniconda3/env/anomalib_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np  \n",
    "from matplotlib import pyplot as plt \n",
    "from PIL import Image    \n",
    "import torch\n",
    "\n",
    "from anomalib.config import get_configurable_parameters\n",
    "from anomalib.data import get_datamodule\n",
    "from anomalib.models import get_model\n",
    "from anomalib.models.components import feature_extractors\n",
    "import torchvision\n",
    "from anomalib.models.components.feature_extractors import TorchFXFeatureExtractor\n",
    "from torchvision.models.densenet import DenseNet201_Weights\n",
    "import torch.nn.functional as F\n",
    "from anomalib.models.components.cluster.kmeans import KMeans\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset:\n",
      "  name: airogs\n",
      "  format: airogs\n",
      "  path: /home/students/tyang/yolov5results\n",
      "  task: classification # options: [classification, segmentation]\n",
      "  category: cat0/crops/od\n",
      "  pre_selection: True\n",
      "  number_of_samples: 37\n",
      "  train_batch_size: 36\n",
      "  eval_batch_size: 36\n",
      "  num_workers: 8\n",
      "  image_size: 256 # dimensions to which images are resized (mandatory)\n",
      "  center_crop:  # dimensions to which images are center-cropped after resizing (optional)\n",
      "  normalization: imagenet # data distribution to which the images will be normalized: [none, imagenet]\n",
      "  transform_config:\n",
      "    train: null\n",
      "    eval: null\n",
      "  test_split_mode: from_dir # options: [from_dir, synthetic]\n",
      "  test_split_ratio: 0.025 # fraction of train images held out testing (usage depends on test_split_mode)\n",
      "  val_split_mode: same_as_test # options: [same_as_test, from_test, synthetic]\n",
      "  val_split_ratio: 0.025 # fraction of train/test images held out for validation (usage depends on val_split_mode)\n",
      "\n",
      "  tiling:\n",
      "    apply: false\n",
      "    tile_size: null\n",
      "    stride: null\n",
      "    remove_border_count: 0\n",
      "    use_random_tiling: False\n",
      "    random_tile_count: 16\n",
      "\n",
      "model:\n",
      "  name: patchcore\n",
      "  backbone: wide_resnet50_2\n",
      "  pre_trained: true\n",
      "  layers:\n",
      "    - layer2\n",
      "    - layer3\n",
      "    \n",
      "  coreset_sampling_ratio: 0.2\n",
      "\n",
      "  num_neighbors: 5\n",
      "  normalization_method: min_max # options: [null, min_max, cdf]\n",
      "\n",
      "metrics:\n",
      "  image:\n",
      "    - F1Score\n",
      "    - AUROC\n",
      "  pixel:\n",
      "    - F1Score\n",
      "    - AUROC\n",
      "  threshold:\n",
      "    method: adaptive #options: [adaptive, manual]\n",
      "    manual_image: null\n",
      "    manual_pixel: null\n",
      "\n",
      "visualization:\n",
      "  show_images: False # show images on the screen\n",
      "  save_images: True # save images to the file system\n",
      "  log_images: True # log images to the available loggers (if any)\n",
      "  image_save_path: null # path to which images will be saved\n",
      "  mode: full # options: [\"full\", \"simple\"]\n",
      "\n",
      "project:\n",
      "  seed: 0\n",
      "  path: /work/scratch/tyang/anomalib/\n",
      "\n",
      "logging:\n",
      "  logger: [] # options: [comet, tensorboard, wandb, csv] or combinations.\n",
      "  log_graph: false # Logs the model graph to respective logger.\n",
      "\n",
      "optimization:\n",
      "  export_mode: null # options: onnx, openvino\n",
      "\n",
      "# PL Trainer Args. Don't add extra parameter here.\n",
      "trainer:\n",
      "  enable_checkpointing: true\n",
      "  default_root_dir: null\n",
      "  gradient_clip_val: 0\n",
      "  gradient_clip_algorithm: norm\n",
      "  num_nodes: 1\n",
      "  devices: 1\n",
      "  enable_progress_bar: true\n",
      "  overfit_batches: 0.0\n",
      "  track_grad_norm: -1\n",
      "  check_val_every_n_epoch: 1 # Don't validate before extracting features.\n",
      "  fast_dev_run: false\n",
      "  accumulate_grad_batches: 1\n",
      "  max_epochs: 1\n",
      "  min_epochs: null\n",
      "  max_steps: -1\n",
      "  min_steps: null\n",
      "  max_time: null\n",
      "  limit_train_batches: 1.0\n",
      "  limit_val_batches: 1.0\n",
      "  limit_test_batches: 1.0\n",
      "  limit_predict_batches: 1.0\n",
      "  val_check_interval: 1.0 # Don't validate before extracting features.\n",
      "  log_every_n_steps: 50\n",
      "  accelerator: auto # <\"cpu\", \"gpu\", \"tpu\", \"ipu\", \"hpu\", \"auto\">\n",
      "  strategy: null\n",
      "  sync_batchnorm: false\n",
      "  precision: 32\n",
      "  enable_model_summary: true\n",
      "  num_sanity_val_steps: 0\n",
      "  profiler: null\n",
      "  benchmark: false\n",
      "  deterministic: false\n",
      "  reload_dataloaders_every_n_epochs: 0\n",
      "  auto_lr_find: false\n",
      "  replace_sampler_ddp: true\n",
      "  detect_anomaly: false\n",
      "  auto_scale_batch_size: false\n",
      "  plugins: null\n",
      "  move_metrics_to_cpu: false\n",
      "  multiple_trainloader_mode: max_size_cycle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/students/tyang/anomalib/src/anomalib/config/config.py:243: UserWarning: The seed value is now fixed to 0. Up to v0.3.7, the seed was not fixed when the seed value was set to 0. If you want to use the random seed, please select `None` for the seed value (`null` in the YAML file) or remove the `seed` key from the YAML file.\n",
      "  warn(\n",
      "/home/students/tyang/anomalib/src/anomalib/config/config.py:280: UserWarning: config.project.unique_dir is set to False. This does not ensure that your results will be written in an empty directory and you may overwrite files.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "MODEL = \"patchcore\"\n",
    "CONFIG_PATH = f\"/home/students/tyang/anomalib/src/anomalib/models/{MODEL}/config_select.yaml\"\n",
    "with open(file=CONFIG_PATH, mode=\"r\",encoding=\"utf-8\") as f:\n",
    "    print(f.read())\n",
    "    \n",
    "config = get_configurable_parameters(config_path=CONFIG_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/students/tyang/anomalib/src/anomalib/data/utils/split.py:110: UserWarning: Zero subset length encountered during splitting. This means one of your subsets might be empty or devoid of either normal or anomalous images.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data_module = get_datamodule(config=config)\n",
    "data_module.prepare_data() # check if the dataset is avaliable\n",
    "data_module.setup()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "i, train_data = next(enumerate(data_module.train_dataloader()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/scratch/tyang/miniconda3/env/anomalib_env/lib/python3.10/site-packages/torch/overrides.py:110: UserWarning: 'has_cuda' is deprecated, please use 'torch.backends.cuda.is_built()'\n",
      "  torch.has_cuda,\n",
      "/work/scratch/tyang/miniconda3/env/anomalib_env/lib/python3.10/site-packages/torch/overrides.py:111: UserWarning: 'has_cudnn' is deprecated, please use 'torch.backends.cudnn.is_available()'\n",
      "  torch.has_cudnn,\n",
      "/work/scratch/tyang/miniconda3/env/anomalib_env/lib/python3.10/site-packages/torch/overrides.py:117: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  torch.has_mps,\n",
      "/work/scratch/tyang/miniconda3/env/anomalib_env/lib/python3.10/site-packages/torch/overrides.py:118: UserWarning: 'has_mkldnn' is deprecated, please use 'torch.backends.mkldnn.is_available()'\n",
      "  torch.has_mkldnn,\n"
     ]
    }
   ],
   "source": [
    "\n",
    "feature_extractor = TorchFXFeatureExtractor(\n",
    "                    backbone=\"densenet201\",\n",
    "                    return_nodes=[\"features.denseblock1.denselayer6.conv2\"],\n",
    "                    weights=DenseNet201_Weights.IMAGENET1K_V1,\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([36, 32, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "# feature shape is (batch_size, channel, height, width) \n",
    "feature = feature_extractor(train_data[\"image\"])\n",
    "print(feature[\"features.denseblock1.denselayer6.conv2\"].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([36])\n"
     ]
    }
   ],
   "source": [
    "print(train_data[\"label\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract features from the whol training dataset\n",
    "\n",
    "feature_list = []\n",
    "\n",
    "for  i, train_data in enumerate(data_module.train_dataloader()):\n",
    "    features = feature_extractor(train_data[\"image\"])[\"features.denseblock1.denselayer6.conv2\"]\n",
    "    feature_list.append(features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the list of features to a tensor\n",
    "global_feature_tensor = torch.vstack(feature_list)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([37, 32, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "# global feature shape is (train_data size, channel, height, width)\n",
    "print(global_feature_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anomalib.models.components.cluster.kmeans import KMeans\n",
    "\n",
    "\n",
    "def calculate_WSS(points, kmax):\n",
    "    feature_t = points.permute(1,0,2,3)\n",
    "    feature_t = feature_t.flatten(start_dim=1)\n",
    "    feature_t= feature_t.permute(1,0)\n",
    "    sse = []\n",
    "    for k in range(1, kmax+1):\n",
    "        kmeans = KMeans(n_clusters = k)\n",
    "        kmeans.fit(feature_t)\n",
    "        centroids = kmeans.cluster_centers_\n",
    "        pred_clusters = kmeans.predict(feature_t)\n",
    "        curr_sse = 0\n",
    "\n",
    "        # calculate square of Euclidean distance of each point from its cluster center and add to current WSS\n",
    "        for i in range(len(points)):\n",
    "            curr_center = centroids[pred_clusters[i]]\n",
    "            curr_sse += (points[i, 0] - curr_center[0]) ** 2 + (points[i, 1] - curr_center[1]) ** 2\n",
    "\n",
    "        sse.append(curr_sse)\n",
    "    return sse\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sse = calculate_WSS(global_feature_tensor, 10)\n",
    "plt.plot(range(1,11),sse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[0.1960, 0.2782, 0.2381,  ..., 0.3279, 0.3385, 0.3428],\n",
      "        [0.5216, 0.6458, 0.6314,  ..., 0.7007, 0.7394, 0.3574],\n",
      "        [0.2896, 0.3421, 0.4110,  ..., 0.4412, 0.6790, 0.3689],\n",
      "        ...,\n",
      "        [0.2940, 0.4983, 0.4169,  ..., 0.7668, 0.4906, 0.2321],\n",
      "        [0.2996, 0.5835, 0.6853,  ..., 0.5636, 0.5120, 0.7900],\n",
      "        [0.2136, 0.2526, 0.3956,  ..., 0.3296, 0.3633, 0.3693]]), tensor([[0.3805, 0.3957, 0.4100,  ..., 0.4980, 0.4825, 0.4664],\n",
      "        [0.7395, 0.8277, 0.7294,  ..., 0.9209, 0.9178, 0.5110],\n",
      "        [0.3917, 0.5997, 0.6703,  ..., 0.6045, 0.8694, 0.5385],\n",
      "        ...,\n",
      "        [0.5174, 0.6140, 0.5915,  ..., 0.9444, 0.6319, 0.4015],\n",
      "        [0.4792, 0.7655, 0.6755,  ..., 0.6736, 0.6027, 0.9900],\n",
      "        [0.3413, 0.4602, 0.4998,  ..., 0.5391, 0.5212, 0.5331]]), tensor([[0.3798, 0.3875, 0.4002,  ..., 0.5085, 0.4359, 0.4789],\n",
      "        [0.7267, 0.8088, 0.7293,  ..., 0.9125, 0.9269, 0.5003],\n",
      "        [0.4029, 0.5963, 0.6617,  ..., 0.5812, 0.9273, 0.5093],\n",
      "        ...,\n",
      "        [0.5106, 0.6468, 0.6541,  ..., 0.9005, 0.6659, 0.3341],\n",
      "        [0.4974, 0.6997, 0.6981,  ..., 0.6023, 0.7155, 0.9154],\n",
      "        [0.3138, 0.4507, 0.4472,  ..., 0.4831, 0.5133, 0.4808]]), tensor([[0.4024, 0.3534, 0.4200,  ..., 0.5417, 0.3830, 0.3647],\n",
      "        [0.7448, 0.7719, 0.8004,  ..., 0.9202, 0.9730, 0.4340],\n",
      "        [0.3533, 0.5160, 0.5261,  ..., 0.5907, 0.9776, 0.5454],\n",
      "        ...,\n",
      "        [0.5221, 0.6372, 0.5578,  ..., 0.9753, 0.6301, 0.3335],\n",
      "        [0.4336, 0.7435, 0.7374,  ..., 0.7406, 0.7174, 0.8302],\n",
      "        [0.2162, 0.3368, 0.3671,  ..., 0.3477, 0.4827, 0.3850]]), tensor([[0.4097, 0.3310, 0.4045,  ..., 0.5326, 0.3662, 0.3679],\n",
      "        [0.7353, 0.7580, 0.8435,  ..., 0.9765, 0.9206, 0.4258],\n",
      "        [0.3468, 0.5284, 0.5250,  ..., 0.5907, 1.0183, 0.4729],\n",
      "        ...,\n",
      "        [0.5219, 0.5780, 0.5104,  ..., 0.9970, 0.5934, 0.3260],\n",
      "        [0.4372, 0.8084, 0.6617,  ..., 0.7552, 0.7681, 0.7377],\n",
      "        [0.1872, 0.3452, 0.3821,  ..., 0.3428, 0.4869, 0.3206]]), tensor([[0.4359, 0.3886, 0.4451,  ..., 0.6133, 0.3774, 0.3922],\n",
      "        [0.8082, 0.7251, 0.8904,  ..., 0.9647, 1.0092, 0.4451],\n",
      "        [0.3864, 0.5797, 0.5514,  ..., 0.6270, 1.0103, 0.5342],\n",
      "        ...,\n",
      "        [0.5436, 0.6095, 0.5421,  ..., 1.0557, 0.5598, 0.3721],\n",
      "        [0.4044, 0.9162, 0.6995,  ..., 0.8482, 0.7376, 0.8164],\n",
      "        [0.2306, 0.3808, 0.4811,  ..., 0.4175, 0.5022, 0.3870]]), tensor([[0.1801, 0.2220, 0.2230,  ..., 0.3371, 0.3524, 0.4463],\n",
      "        [0.4186, 0.5817, 0.5970,  ..., 0.6429, 0.6492, 0.3105],\n",
      "        [0.2235, 0.3628, 0.4498,  ..., 0.4382, 0.6375, 0.2817],\n",
      "        ...,\n",
      "        [0.2765, 0.4511, 0.4367,  ..., 0.7768, 0.4770, 0.2302],\n",
      "        [0.2888, 0.6129, 0.6691,  ..., 0.5877, 0.5448, 0.6859],\n",
      "        [0.2490, 0.3937, 0.5397,  ..., 0.4572, 0.5144, 0.3583]]), tensor([[0.2136, 0.2544, 0.2672,  ..., 0.3916, 0.3638, 0.4653],\n",
      "        [0.4431, 0.5910, 0.6332,  ..., 0.6633, 0.6728, 0.3124],\n",
      "        [0.2367, 0.4021, 0.4876,  ..., 0.4731, 0.7010, 0.3196],\n",
      "        ...,\n",
      "        [0.2881, 0.4891, 0.4148,  ..., 0.7853, 0.4763, 0.2614],\n",
      "        [0.2796, 0.6537, 0.6653,  ..., 0.6200, 0.5431, 0.7280],\n",
      "        [0.2653, 0.4250, 0.5819,  ..., 0.4766, 0.5180, 0.3598]]), tensor([[0.2889, 0.3148, 0.3069,  ..., 0.4303, 0.3355, 0.4023],\n",
      "        [0.5189, 0.6506, 0.6337,  ..., 0.7742, 0.7724, 0.3456],\n",
      "        [0.2618, 0.4345, 0.4966,  ..., 0.5242, 0.8255, 0.3933],\n",
      "        ...,\n",
      "        [0.3408, 0.5573, 0.4393,  ..., 0.7821, 0.5260, 0.2453],\n",
      "        [0.2842, 0.6800, 0.6977,  ..., 0.6183, 0.5793, 0.7351],\n",
      "        [0.2015, 0.3190, 0.4897,  ..., 0.3790, 0.4333, 0.3126]]), tensor([[0.2138, 0.3010, 0.2393,  ..., 0.3925, 0.3109, 0.3967],\n",
      "        [0.4651, 0.5905, 0.6457,  ..., 0.7035, 0.6690, 0.2906],\n",
      "        [0.2763, 0.3419, 0.4697,  ..., 0.4739, 0.7729, 0.3205],\n",
      "        ...,\n",
      "        [0.3035, 0.4628, 0.4369,  ..., 0.8524, 0.4598, 0.2160],\n",
      "        [0.2696, 0.6529, 0.6362,  ..., 0.6183, 0.6244, 0.6229],\n",
      "        [0.1617, 0.3248, 0.4882,  ..., 0.3566, 0.4674, 0.2569]]), tensor([[0.2242, 0.3071, 0.2634,  ..., 0.3931, 0.3688, 0.4177],\n",
      "        [0.5250, 0.6319, 0.6670,  ..., 0.6997, 0.7408, 0.3625],\n",
      "        [0.2864, 0.4152, 0.4908,  ..., 0.4973, 0.7076, 0.3873],\n",
      "        ...,\n",
      "        [0.2995, 0.5283, 0.4244,  ..., 0.8282, 0.4739, 0.2749],\n",
      "        [0.2877, 0.6673, 0.6470,  ..., 0.6160, 0.5370, 0.8115],\n",
      "        [0.2598, 0.3602, 0.5246,  ..., 0.4194, 0.4593, 0.3985]]), tensor([[0.2289, 0.3289, 0.2468,  ..., 0.4443, 0.3357, 0.4531],\n",
      "        [0.4900, 0.6179, 0.6996,  ..., 0.7101, 0.6802, 0.3099],\n",
      "        [0.2896, 0.3769, 0.5286,  ..., 0.4764, 0.7959, 0.3355],\n",
      "        ...,\n",
      "        [0.3003, 0.4969, 0.4363,  ..., 0.8969, 0.4658, 0.2677],\n",
      "        [0.2722, 0.7495, 0.6251,  ..., 0.6668, 0.6354, 0.6732],\n",
      "        [0.2037, 0.3820, 0.5854,  ..., 0.4273, 0.5293, 0.3052]]), tensor([[0.2523, 0.3183, 0.2770,  ..., 0.4096, 0.3472, 0.4101],\n",
      "        [0.5078, 0.6196, 0.6528,  ..., 0.7351, 0.7395, 0.3431],\n",
      "        [0.2783, 0.4193, 0.4954,  ..., 0.5381, 0.7662, 0.3801],\n",
      "        ...,\n",
      "        [0.3370, 0.5122, 0.4564,  ..., 0.8577, 0.4909, 0.2421],\n",
      "        [0.2914, 0.6614, 0.6648,  ..., 0.6259, 0.6102, 0.7098],\n",
      "        [0.2111, 0.3493, 0.5004,  ..., 0.3811, 0.4695, 0.3220]]), tensor([[0.3534, 0.3639, 0.3852,  ..., 0.4560, 0.4306, 0.4961],\n",
      "        [0.6946, 0.7509, 0.7275,  ..., 0.8524, 0.8555, 0.4657],\n",
      "        [0.3440, 0.5626, 0.5527,  ..., 0.5724, 0.8915, 0.4993],\n",
      "        ...,\n",
      "        [0.4259, 0.5363, 0.5329,  ..., 0.9416, 0.6165, 0.3389],\n",
      "        [0.3910, 0.7463, 0.6506,  ..., 0.6700, 0.6309, 0.9547],\n",
      "        [0.3248, 0.4387, 0.5408,  ..., 0.5350, 0.4747, 0.5442]]), tensor([[0.2412, 0.2919, 0.2807,  ..., 0.4174, 0.3623, 0.4293],\n",
      "        [0.5004, 0.6180, 0.6880,  ..., 0.7094, 0.7267, 0.3371],\n",
      "        [0.2789, 0.4189, 0.4937,  ..., 0.5325, 0.7346, 0.3659],\n",
      "        ...,\n",
      "        [0.3251, 0.5074, 0.4144,  ..., 0.8566, 0.4634, 0.2760],\n",
      "        [0.3068, 0.6797, 0.6795,  ..., 0.6471, 0.5676, 0.7500],\n",
      "        [0.2443, 0.3956, 0.5439,  ..., 0.4239, 0.4910, 0.3530]]), tensor([[0.2472, 0.2831, 0.2678,  ..., 0.3979, 0.3483, 0.4346],\n",
      "        [0.4734, 0.6112, 0.6338,  ..., 0.7007, 0.7002, 0.3246],\n",
      "        [0.2502, 0.4105, 0.4882,  ..., 0.5186, 0.7665, 0.3620],\n",
      "        ...,\n",
      "        [0.3057, 0.5087, 0.4544,  ..., 0.8122, 0.4983, 0.2433],\n",
      "        [0.2857, 0.6534, 0.6611,  ..., 0.6085, 0.5795, 0.7078],\n",
      "        [0.2261, 0.3758, 0.5361,  ..., 0.4245, 0.4850, 0.3303]]), tensor([[0.2595, 0.3080, 0.2983,  ..., 0.4497, 0.3448, 0.4132],\n",
      "        [0.5245, 0.6366, 0.6974,  ..., 0.7486, 0.7498, 0.3350],\n",
      "        [0.2767, 0.4246, 0.5062,  ..., 0.5211, 0.7884, 0.3751],\n",
      "        ...,\n",
      "        [0.3290, 0.5271, 0.3946,  ..., 0.8480, 0.4609, 0.2816],\n",
      "        [0.2912, 0.7143, 0.6756,  ..., 0.6644, 0.5633, 0.7498],\n",
      "        [0.2173, 0.3677, 0.5328,  ..., 0.4032, 0.4659, 0.3217]]), tensor([[0.2739, 0.3189, 0.3017,  ..., 0.4419, 0.3680, 0.4496],\n",
      "        [0.5202, 0.6406, 0.6595,  ..., 0.7460, 0.7586, 0.3584],\n",
      "        [0.2703, 0.4550, 0.5301,  ..., 0.5253, 0.7817, 0.3892],\n",
      "        ...,\n",
      "        [0.3251, 0.5591, 0.4405,  ..., 0.8210, 0.4994, 0.2816],\n",
      "        [0.2874, 0.7143, 0.6680,  ..., 0.6423, 0.5673, 0.7770],\n",
      "        [0.2507, 0.3905, 0.5682,  ..., 0.4482, 0.4957, 0.3675]]), tensor([[0.2780, 0.3381, 0.3147,  ..., 0.3884, 0.4085, 0.4500],\n",
      "        [0.5815, 0.6177, 0.6632,  ..., 0.7583, 0.7280, 0.3844],\n",
      "        [0.3012, 0.4568, 0.5017,  ..., 0.5358, 0.7971, 0.4079],\n",
      "        ...,\n",
      "        [0.3553, 0.5006, 0.4958,  ..., 0.8601, 0.5368, 0.2554],\n",
      "        [0.2955, 0.6622, 0.6310,  ..., 0.6392, 0.6314, 0.7666],\n",
      "        [0.2432, 0.3762, 0.4886,  ..., 0.4169, 0.4913, 0.3850]]), tensor([[0.2628, 0.2899, 0.2779,  ..., 0.4157, 0.3517, 0.4434],\n",
      "        [0.4741, 0.6346, 0.6573,  ..., 0.7175, 0.7277, 0.3373],\n",
      "        [0.2434, 0.4279, 0.4831,  ..., 0.5415, 0.7739, 0.3676],\n",
      "        ...,\n",
      "        [0.3492, 0.5059, 0.4628,  ..., 0.8395, 0.5387, 0.2457],\n",
      "        [0.3052, 0.6776, 0.7202,  ..., 0.6336, 0.6298, 0.6919],\n",
      "        [0.2300, 0.3726, 0.5320,  ..., 0.4139, 0.4916, 0.3330]])]\n",
      "20\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/scratch/tyang/miniconda3/env/anomalib_env/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "/work/scratch/tyang/miniconda3/env/anomalib_env/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m,\u001b[38;5;241m20\u001b[39m):\n\u001b[1;32m     13\u001b[0m     kmeans_fit \u001b[38;5;241m=\u001b[39m KMeans(n_clusters \u001b[38;5;241m=\u001b[39m i)\u001b[38;5;241m.\u001b[39mfit(feature_t)\n\u001b[0;32m---> 14\u001b[0m     silhouette_avg\u001b[38;5;241m.\u001b[39mappend(\u001b[43msilhouette_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkmeans_fit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabels_\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     17\u001b[0m x_ticks \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m10\u001b[39m,\u001b[38;5;241m20\u001b[39m,\u001b[38;5;241m11\u001b[39m)\n\u001b[1;32m     18\u001b[0m plt\u001b[38;5;241m.\u001b[39mxticks(x_ticks)\n",
      "File \u001b[0;32m/work/scratch/tyang/miniconda3/env/anomalib_env/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:214\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    210\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    211\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    212\u001b[0m         )\n\u001b[1;32m    213\u001b[0m     ):\n\u001b[0;32m--> 214\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    224\u001b[0m     )\n",
      "File \u001b[0;32m/work/scratch/tyang/miniconda3/env/anomalib_env/lib/python3.10/site-packages/sklearn/metrics/cluster/_unsupervised.py:130\u001b[0m, in \u001b[0;36msilhouette_score\u001b[0;34m(X, labels, metric, sample_size, random_state, **kwds)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    129\u001b[0m         X, labels \u001b[38;5;241m=\u001b[39m X[indices], labels[indices]\n\u001b[0;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmean(\u001b[43msilhouette_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/work/scratch/tyang/miniconda3/env/anomalib_env/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:187\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    185\u001b[0m global_skip_validation \u001b[38;5;241m=\u001b[39m get_config()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip_parameter_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m global_skip_validation:\n\u001b[0;32m--> 187\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    189\u001b[0m func_sig \u001b[38;5;241m=\u001b[39m signature(func)\n\u001b[1;32m    191\u001b[0m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n",
      "File \u001b[0;32m/work/scratch/tyang/miniconda3/env/anomalib_env/lib/python3.10/site-packages/sklearn/metrics/cluster/_unsupervised.py:282\u001b[0m, in \u001b[0;36msilhouette_samples\u001b[0;34m(X, labels, metric, **kwds)\u001b[0m\n\u001b[1;32m    278\u001b[0m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetric\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m metric\n\u001b[1;32m    279\u001b[0m reduce_func \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(\n\u001b[1;32m    280\u001b[0m     _silhouette_reduce, labels\u001b[38;5;241m=\u001b[39mlabels, label_freqs\u001b[38;5;241m=\u001b[39mlabel_freqs\n\u001b[1;32m    281\u001b[0m )\n\u001b[0;32m--> 282\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpairwise_distances_chunked\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce_func\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreduce_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    283\u001b[0m intra_clust_dists, inter_clust_dists \u001b[38;5;241m=\u001b[39m results\n\u001b[1;32m    284\u001b[0m intra_clust_dists \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate(intra_clust_dists)\n",
      "File \u001b[0;32m/work/scratch/tyang/miniconda3/env/anomalib_env/lib/python3.10/site-packages/sklearn/metrics/pairwise.py:2018\u001b[0m, in \u001b[0;36mpairwise_distances_chunked\u001b[0;34m(X, Y, reduce_func, metric, n_jobs, working_memory, **kwds)\u001b[0m\n\u001b[1;32m   2016\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2017\u001b[0m     X_chunk \u001b[38;5;241m=\u001b[39m X[sl]\n\u001b[0;32m-> 2018\u001b[0m D_chunk \u001b[38;5;241m=\u001b[39m \u001b[43mpairwise_distances\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2019\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (X \u001b[38;5;129;01mis\u001b[39;00m Y \u001b[38;5;129;01mor\u001b[39;00m Y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m PAIRWISE_DISTANCE_FUNCTIONS\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m   2020\u001b[0m     metric, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2021\u001b[0m ) \u001b[38;5;129;01mis\u001b[39;00m euclidean_distances:\n\u001b[1;32m   2022\u001b[0m     \u001b[38;5;66;03m# zeroing diagonal, taking care of aliases of \"euclidean\",\u001b[39;00m\n\u001b[1;32m   2023\u001b[0m     \u001b[38;5;66;03m# i.e. \"l2\"\u001b[39;00m\n\u001b[1;32m   2024\u001b[0m     D_chunk\u001b[38;5;241m.\u001b[39mflat[sl\u001b[38;5;241m.\u001b[39mstart :: _num_samples(X) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m/work/scratch/tyang/miniconda3/env/anomalib_env/lib/python3.10/site-packages/sklearn/metrics/pairwise.py:2196\u001b[0m, in \u001b[0;36mpairwise_distances\u001b[0;34m(X, Y, metric, n_jobs, force_all_finite, **kwds)\u001b[0m\n\u001b[1;32m   2193\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m distance\u001b[38;5;241m.\u001b[39msquareform(distance\u001b[38;5;241m.\u001b[39mpdist(X, metric\u001b[38;5;241m=\u001b[39mmetric, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds))\n\u001b[1;32m   2194\u001b[0m     func \u001b[38;5;241m=\u001b[39m partial(distance\u001b[38;5;241m.\u001b[39mcdist, metric\u001b[38;5;241m=\u001b[39mmetric, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m-> 2196\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_parallel_pairwise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/work/scratch/tyang/miniconda3/env/anomalib_env/lib/python3.10/site-packages/sklearn/metrics/pairwise.py:1766\u001b[0m, in \u001b[0;36m_parallel_pairwise\u001b[0;34m(X, Y, func, n_jobs, **kwds)\u001b[0m\n\u001b[1;32m   1763\u001b[0m X, Y, dtype \u001b[38;5;241m=\u001b[39m _return_float_dtype(X, Y)\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m effective_n_jobs(n_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m-> 1766\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1768\u001b[0m \u001b[38;5;66;03m# enforce a threading backend to prevent data communication overhead\u001b[39;00m\n\u001b[1;32m   1769\u001b[0m fd \u001b[38;5;241m=\u001b[39m delayed(_dist_wrapper)\n",
      "File \u001b[0;32m/work/scratch/tyang/miniconda3/env/anomalib_env/lib/python3.10/site-packages/sklearn/metrics/pairwise.py:338\u001b[0m, in \u001b[0;36meuclidean_distances\u001b[0;34m(X, Y, Y_norm_squared, squared, X_norm_squared)\u001b[0m\n\u001b[1;32m    332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m Y_norm_squared\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m!=\u001b[39m (\u001b[38;5;241m1\u001b[39m, Y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]):\n\u001b[1;32m    333\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    334\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncompatible dimensions for Y of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mY\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    335\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mY_norm_squared of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moriginal_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    336\u001b[0m         )\n\u001b[0;32m--> 338\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_euclidean_distances\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_norm_squared\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_norm_squared\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msquared\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/work/scratch/tyang/miniconda3/env/anomalib_env/lib/python3.10/site-packages/sklearn/metrics/pairwise.py:376\u001b[0m, in \u001b[0;36m_euclidean_distances\u001b[0;34m(X, Y, X_norm_squared, Y_norm_squared, squared)\u001b[0m\n\u001b[1;32m    371\u001b[0m         YY \u001b[38;5;241m=\u001b[39m row_norms(Y, squared\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[np\u001b[38;5;241m.\u001b[39mnewaxis, :]\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mfloat32:\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;66;03m# To minimize precision issues with float32, we compute the distance\u001b[39;00m\n\u001b[1;32m    375\u001b[0m     \u001b[38;5;66;03m# matrix on chunks of X and Y upcast to float64\u001b[39;00m\n\u001b[0;32m--> 376\u001b[0m     distances \u001b[38;5;241m=\u001b[39m \u001b[43m_euclidean_distances_upcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mXX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mYY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    378\u001b[0m     \u001b[38;5;66;03m# if dtype is already float64, no need to chunk and upcast\u001b[39;00m\n\u001b[1;32m    379\u001b[0m     distances \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m safe_sparse_dot(X, Y\u001b[38;5;241m.\u001b[39mT, dense_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/work/scratch/tyang/miniconda3/env/anomalib_env/lib/python3.10/site-packages/sklearn/metrics/pairwise.py:588\u001b[0m, in \u001b[0;36m_euclidean_distances_upcast\u001b[0;34m(X, XX, Y, YY, batch_size)\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    586\u001b[0m     YY_chunk \u001b[38;5;241m=\u001b[39m YY[:, y_slice]\n\u001b[0;32m--> 588\u001b[0m d \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[43msafe_sparse_dot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_chunk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdense_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    589\u001b[0m d \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m XX_chunk\n\u001b[1;32m    590\u001b[0m d \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m YY_chunk\n",
      "File \u001b[0;32m/work/scratch/tyang/miniconda3/env/anomalib_env/lib/python3.10/site-packages/sklearn/utils/extmath.py:195\u001b[0m, in \u001b[0;36msafe_sparse_dot\u001b[0;34m(a, b, dense_output)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    192\u001b[0m     ret \u001b[38;5;241m=\u001b[39m a \u001b[38;5;241m@\u001b[39m b\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m--> 195\u001b[0m     \u001b[43msparse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43missparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m sparse\u001b[38;5;241m.\u001b[39missparse(b)\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m dense_output\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(ret, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoarray\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    199\u001b[0m ):\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39mtoarray()\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "File \u001b[0;32m/work/scratch/tyang/miniconda3/env/anomalib_env/lib/python3.10/site-packages/scipy/sparse/_base.py:1461\u001b[0m, in \u001b[0;36missparse\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1456\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m   1458\u001b[0m sparray\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__doc__\u001b[39m \u001b[38;5;241m=\u001b[39m _spbase\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__doc__\u001b[39m\n\u001b[0;32m-> 1461\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21missparse\u001b[39m(x):\n\u001b[1;32m   1462\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Is `x` of a sparse array type?\u001b[39;00m\n\u001b[1;32m   1463\u001b[0m \n\u001b[1;32m   1464\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1485\u001b[0m \u001b[38;5;124;03m    False\u001b[39;00m\n\u001b[1;32m   1486\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1487\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, _spbase)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "feature_t = global_feature_tensor.permute(1,0,2,3)\n",
    "feature_t = feature_t.flatten(start_dim=1)\n",
    "feature_t= feature_t.permute(1,0)\n",
    "\n",
    "silhouette_avg = []\n",
    "for i in range(10,20):\n",
    "    kmeans_fit = KMeans(n_clusters = i).fit(feature_t)\n",
    "    silhouette_avg.append(silhouette_score(feature_t, kmeans_fit.labels_))\n",
    "\n",
    "\n",
    "x_ticks = np.linspace(10,20,11)\n",
    "plt.xticks(x_ticks)\n",
    "plt.plot(range(10,20), silhouette_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anomalib.models.components.cluster.kmeans import KMeans\n",
    "\n",
    "def get_kmeans_centers(feature_tensor, n_clusters):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        feature_t: feature tensor, shape is (batch_size, channel, height, width)\n",
    "        n_clusters: number of clusters\n",
    "        \n",
    "    Returns:\n",
    "        cluster_center: shape is (n_clusters, channel)\n",
    "        kmeans: kmeans model   \"\"\"\n",
    "    \n",
    "    feature_t = feature_tensor.permute(1,0,2,3)\n",
    "    feature_t = feature_t.flatten(start_dim=1)\n",
    "    feature_t= feature_t.permute(1,0)\n",
    "\n",
    "\n",
    "    kmeans = KMeans(n_clusters=n_clusters)\n",
    "    kmeans.fit(feature_t)\n",
    "    cluster_center = kmeans.cluster_centers_\n",
    "   \n",
    "    return cluster_center, kmeans\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 32])\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "clusters_centers,kmeans = get_kmeans_centers(global_feature_tensor, n_clusters=12)\n",
    "\n",
    "print(clusters_centers.shape)\n",
    "print(len(clusters_centers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words_statistics(Ptst, Cref, S):\n",
    "    \"\"\"\n",
    "    Params:\n",
    "    Ptst: Feature tensor of a set of images, tensor of shape (N, C, H, W)\n",
    "    Cref: reference Cluster centers, tensor of shape (K, C)\n",
    "    S: number of subregions per image dimension, integer\n",
    "\n",
    "    Returns:\n",
    "    bow_stats: list of normalized Bag-of-words statistics, possibility-like , length N, each element is a tensor of shape (S * S, K)\n",
    "    \"\"\"\n",
    "    Ptst = torch.vsplit(Ptst, Ptst.shape[0])\n",
    "    bow_stats = []\n",
    "    for Itst in Ptst:\n",
    "        Itst = Itst.squeeze(0)\n",
    "        #print(Itst.shape)\n",
    "        subtensors = torch.chunk(Itst, S, dim=1)\n",
    "        subtensor = [torch.chunk(st, S, dim=2) for st in subtensors]\n",
    "        \n",
    "        \n",
    "        image_bow_stats = torch.zeros(S * S, len(Cref), dtype=torch.float32)\n",
    "        for i in range(S):\n",
    "            for j in range(S):\n",
    "                st_value = subtensor[i][j]\n",
    "                st_value = st_value.flatten(start_dim=1)\n",
    "                st_value = st_value.permute(1,0)\n",
    "               # print(st_value.shape)\n",
    "                \n",
    "                cluster_idx = kmeans.predict(st_value)\n",
    "                #print(cluster_idx.shape)\n",
    "                cluster_idx = cluster_idx.float()\n",
    "\n",
    "                hist = torch.histc(cluster_idx, bins = len(Cref), min = torch.min(cluster_idx), max = torch.max(cluster_idx))\n",
    "                normalized_hist = hist / torch.sum(hist)\n",
    "                image_bow_stats[i * S + j] = normalized_hist\n",
    "        \n",
    "        bow_stats.append(image_bow_stats)\n",
    "            \n",
    "        \n",
    "    return bow_stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_bow_stats = bag_of_words_statistics(global_feature_tensor, clusters_centers, S=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37\n",
      "torch.Size([16, 12])\n"
     ]
    }
   ],
   "source": [
    "print(len(global_bow_stats))\n",
    "print(global_bow_stats[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def kl_distance(i_hist, j_hist):\n",
    "    \"\"\"Params:\n",
    "    i_hist: bow histogram of image i, tensor of shape (1, s * s, K )\n",
    "    j_hist: bow histogram of image j, tensor of shape (1, s * s, K )\n",
    "\n",
    "    Returns:\n",
    "    kl_dist: kl distance between image i and image j, tensor of shape (1)\n",
    "    \"\"\"\n",
    "    i_hist[i_hist == 0] = 1e-10\n",
    "    j_hist[j_hist == 0] = 1e-10\n",
    "    kl_divegence = F.kl_div(j_hist.log(), i_hist,reduction=\"none\")\n",
    "    \n",
    "    sum = torch.sum(kl_divegence,dim=-1)\n",
    "    topk,ids = torch.topk(sum, k=11, dim=0, largest=False, sorted=True)\n",
    "   \n",
    "    dist = torch.mean(topk)\n",
    "  \n",
    "    return dist\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_select(bow_stats, select_ratio, step_size):\n",
    "    \"\"\"\n",
    "    Params:\n",
    "    bow_stats: list of normalized Bag-of-words statistics, possibility-like , length N, each element is a tensor of shape (S * S, K)\n",
    "    select_ratio: ratio of selected images compare to all images, float\n",
    "    step_size: batch size, used for big taining dataset, integer\n",
    "\n",
    "    Returns:\n",
    "    selected_idx: list of selected image indices with highest kl-divergence in each batch, length number of batches, each element has the size of batch_size * select_ratio\n",
    "    \"\"\"\n",
    "    \n",
    "    stacked_bow_stats = torch.vstack(bow_stats)\n",
    "    stacked_bow_stats_reshaped = stacked_bow_stats.view(len(bow_stats),1,bow_stats[0].shape[0],bow_stats[0].shape[1])\n",
    "    stacked_bow_stats_transposed = stacked_bow_stats_reshaped.permute(1,0,2,3)\n",
    "\n",
    "    stacked_bow_stats_reshaped[stacked_bow_stats_reshaped == 0 ] = 1e-10\n",
    "    stacked_bow_stats_transposed[stacked_bow_stats_transposed == 0 ] = 1e-10\n",
    "\n",
    "    selected_idxs =[]\n",
    "    selected_distances = []\n",
    "    for i in range(0, stacked_bow_stats_reshaped.shape[0], step_size):\n",
    "        current_bow = stacked_bow_stats_reshaped[i:i+step_size if i+step_size < stacked_bow_stats_reshaped.shape[0] else stacked_bow_stats_reshaped.shape[0], :, :, :]\n",
    "        kl_divegence = F.kl_div(current_bow.log(), stacked_bow_stats_transposed,reduction=\"none\")\n",
    "       # print(kl_divegence.shape)\n",
    "\n",
    "        sum = torch.sum(kl_divegence,dim=3)\n",
    "        #print(sum.shape)\n",
    "        #topk,ids = torch.topk(sum, k=11, dim=2, largest=False, sorted=True)\n",
    "        dist_matrix = torch.mean(sum,dim=2)\n",
    "        #print(dist_matrix.shape)\n",
    "  \n",
    "        dist_l = torch.sum(dist_matrix,dim=1)\n",
    "        topk_far_distances,f_idx = torch.topk(dist_l, k=int(len(dist_l) * select_ratio), dim=0, largest=True, sorted=True)\n",
    "        #topk_near_distances,n_idx = torch.topk(dist_l, k=int(len(dist_l) * select_ratio ), dim=0, largest=False, sorted=True)\n",
    "        #selected_idx = torch.cat((f_idx,n_idx),dim=0)\n",
    "        selected_idxs.append(f_idx)\n",
    "        #selected_distance = torch.cat((topk_far_distances,topk_near_distances),dim=0)\n",
    "        selected_distances.append(topk_far_distances)\n",
    "\n",
    "    \n",
    "    return selected_idxs, selected_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_select(bow_stats, select_ratio, step_size):\n",
    "    \"\"\"\n",
    "    Params:\n",
    "    bow_stats: list of normalized Bag-of-words statistics, possibility-like , length N, each element is a tensor of shape (S * S, K)\n",
    "    select_ratio: ratio of selected images compare to all images, float\n",
    "    step_size: batch size, used for big taining dataset, integer\n",
    "\n",
    "    Returns:\n",
    "    selected_idx: list of selected image indices with highest kl-divergence in each batch, length number of batches, each element has the size of batch_size * select_ratio\n",
    "    \"\"\"\n",
    "    \n",
    "    stacked_bow_stats = torch.vstack(bow_stats)\n",
    "    stacked_bow_stats_reshaped = stacked_bow_stats.view(len(bow_stats), 1, bow_stats[0].shape[0], bow_stats[0].shape[1])\n",
    "    stacked_bow_stats_transposed = stacked_bow_stats_reshaped.permute(1,0,2,3)\n",
    "\n",
    "    stacked_bow_stats_reshaped[stacked_bow_stats_reshaped == 0 ] = 1e-10\n",
    "    stacked_bow_stats_transposed[stacked_bow_stats_transposed == 0 ] = 1e-10\n",
    "\n",
    "    selected_idxs =[]\n",
    "    selected_distances = []\n",
    "    for i in range(0, stacked_bow_stats_reshaped.shape[0], step_size):\n",
    "        current_bow = stacked_bow_stats_reshaped[i:i+step_size if i+step_size < stacked_bow_stats_reshaped.shape[0] else stacked_bow_stats_reshaped.shape[0], :, :, :]\n",
    "        kl_divegence = F.kl_div(current_bow.log(), stacked_bow_stats_transposed,reduction=\"none\")\n",
    "\n",
    "        sum = torch.sum(kl_divegence,dim=3)\n",
    "        dist_matrix = torch.mean(sum,dim=2)\n",
    "  \n",
    "        dist_l = torch.sum(dist_matrix,dim=1)\n",
    "        topk_far_distances,f_idx = torch.topk(dist_l, k=int(len(dist_l) * select_ratio), dim=0, largest=True, sorted=True)\n",
    "        selected_idxs.append(f_idx)\n",
    "        selected_distances.append(topk_far_distances)\n",
    "\n",
    "    \n",
    "    return selected_idxs, selected_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs,distances = kl_select(global_bow_stats, select_ratio=0.3, step_size=36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "torch.Size([10])\n",
      "10\n",
      "torch.Size([0])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "print(len(idxs))\n",
    "print(idxs[0].shape)\n",
    "print(idxs[0].shape[0])\n",
    "print(idxs[1].shape)\n",
    "stack_distances = torch.cat(distances)\n",
    "print(stack_distances.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def second_selection(index, distance, select_ratio):\n",
    "    \"\"\"\n",
    "    Params:\n",
    "    idxs: list of selected image indices in each batch , length number of batches\n",
    "    distances: list of selected image distances, length number of batches\n",
    "    select_ratio: ratio of selected images compare to all images, float\n",
    "\n",
    "    Returns:\n",
    "    selected_idx: list of selected image indices with biggest distances over all in each batches, length number of batches\n",
    "    \"\"\"\n",
    "    selected_distances = [[] for _ in range(len(distance))]\n",
    "    distance_t = torch.cat(distance)\n",
    "    topk, global_ids = torch.topk(distance_t, k=int(len(distance_t) * select_ratio ), dim=0, largest=True, sorted=True)\n",
    "    selected_idxs = [[] for _ in range(len(index))]\n",
    "    \n",
    "    for global_id in global_ids:\n",
    "        batch_id = global_id // index[0].shape[0]\n",
    "        local_id = global_id % index[0].shape[0]\n",
    "        selected_idxs[batch_id].append(index[batch_id][local_id])\n",
    "        selected_distances[batch_id].append(distance_t[global_id])\n",
    "        \n",
    "  \n",
    "    \n",
    "    return selected_idxs, selected_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_indices,second_dists = second_selection(idxs, distances, select_ratio=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2\n",
      "[tensor(5), tensor(7), tensor(12), tensor(13), tensor(27), tensor(29), tensor(8), tensor(30)]\n",
      "8\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(len(second_indices))\n",
    "print(len(second_dists))\n",
    "print(second_indices[0])\n",
    "print(len(second_indices[0]))\n",
    "print(len(second_indices[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/students/tyang/yolov5results/cat0/crops/od/TRAIN003406.jpg', '/home/students/tyang/yolov5results/cat0/crops/od/TRAIN004690.jpg', '/home/students/tyang/yolov5results/cat0/crops/od/TRAIN006277.jpg', '/home/students/tyang/yolov5results/cat0/crops/od/TRAIN006776.jpg', '/home/students/tyang/yolov5results/cat0/crops/od/TRAIN013504.jpg', '/home/students/tyang/yolov5results/cat0/crops/od/TRAIN015091.jpg', '/home/students/tyang/yolov5results/cat0/crops/od/TRAIN005029.jpg', '/home/students/tyang/yolov5results/cat0/crops/od/TRAIN015602.jpg']\n"
     ]
    }
   ],
   "source": [
    "merged_datas = {\"image_path\": [], \"label\": []}\n",
    "for  i, train_data in enumerate(data_module.train_dataloader()):\n",
    "    selected_data = ( {\"image_path\": train_data[\"image_path\"][second_indice], \"label\": train_data[\"label\"][second_indice]} for second_indice in second_indices[i] )\n",
    "    \n",
    "    \n",
    "    for data in selected_data:\n",
    "        for key, values in data.items():\n",
    "            merged_datas[key].append(values)\n",
    "    \n",
    "\n",
    "print(merged_datas[\"image_path\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "print(len(merged_datas[\"image_path\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv \n",
    "\n",
    "csv_path = \"/home/students/tyang/Documents/category0_dissimiliar8.csv\"\n",
    "\n",
    "\n",
    "with open(csv_path, mode=\"w\", newline=\"\") as csv_file:\n",
    "        fieldnames = [\"image_path\", \"label\"]\n",
    "        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)\n",
    "        for i in range(len(merged_datas[\"image_path\"])):\n",
    "            rowdict = {\"image_path\": merged_datas[\"image_path\"][i], \"label\": merged_datas[\"label\"][i]}\n",
    "            writer.writerow(rowdict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
